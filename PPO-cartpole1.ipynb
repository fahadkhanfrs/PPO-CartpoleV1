{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "139551cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "859a0f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0162af58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy and value model\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU())\n",
    "        \n",
    "        self.policy_layers= nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim))\n",
    "        \n",
    "        self.value_layers = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1))\n",
    "        \n",
    "    def value(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        value = self.value_layers(z)\n",
    "        return value\n",
    "    \n",
    "    def policy(self, obs):\n",
    "        z = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(z)\n",
    "        return policy_logits\n",
    "\n",
    "    def forward(self, obs):\n",
    "        obs = self.shared_layers(obs)\n",
    "        policy_logits = self.policy_layers(obs)\n",
    "        value = self.value_layers(obs)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c7ee00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create trainer\n",
    "class PPOTrainer():\n",
    "    def __init__(self,\n",
    "                 actor_critic,\n",
    "                 ppo_clip_value=0.2,\n",
    "                 target_kl_div=0.01,\n",
    "                 max_policy_train_iters=80,\n",
    "                 value_train_iters=80,\n",
    "                 policy_lr=3e-4,\n",
    "                 value_lr=1e-2):\n",
    "        self.ac = actor_critic\n",
    "        self.ppo_clip_val = ppo_clip_value\n",
    "        self.target_kl_div = target_kl_div\n",
    "        self.max_policy_train_iters = max_policy_train_iters\n",
    "        self.value_train_iters = value_train_iters\n",
    "\n",
    "        policy_params = list(self.ac.shared_layers.parameters()) + list(self.ac.policy_layers.parameters())\n",
    "        self.policy_optim = optim.Adam(policy_params, lr=policy_lr)\n",
    "\n",
    "        value_params = list(self.ac.shared_layers.parameters()) + list(self.ac.value_layers.parameters())\n",
    "        self.value_optim = optim.Adam(value_params, lr=value_lr)\n",
    "\n",
    "    def train_policy(self, obs, acts, old_log_probs, gaes):\n",
    "        for _ in range(self.max_policy_train_iters):\n",
    "            self.policy_optim.zero_grad()\n",
    "\n",
    "            new_logits = self.ac.policy(obs)\n",
    "            new_logits = Categorical(logits=new_logits)\n",
    "            new_log_probs = new_logits.log_prob(acts)\n",
    "\n",
    "            policy_ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            clipped_ratio = policy_ratio.clamp(1 - self.ppo_clip_val, 1 + self.ppo_clip_val)\n",
    "\n",
    "            clipped_loss = clipped_ratio * gaes\n",
    "            full_loss = policy_ratio * gaes\n",
    "            policy_loss = -torch.min(clipped_loss, full_loss).mean()\n",
    "\n",
    "            policy_loss.backward()\n",
    "            self.policy_optim.step()\n",
    "\n",
    "            kl_div = (old_log_probs - new_log_probs).mean()\n",
    "            if kl_div > 1.5 * self.target_kl_div:\n",
    "                break\n",
    "\n",
    "    def train_value(self, obs, returns):\n",
    "        for _ in range(self.value_train_iters):\n",
    "            self.value_optim.zero_grad()\n",
    "\n",
    "            values = self.ac.value(obs)\n",
    "            value_loss = nn.MSELoss()(values, returns)\n",
    "\n",
    "            value_loss.backward()\n",
    "            self.value_optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c9957817",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility functions\n",
    "def discount_rewards(rewards, gamma=0.99):\n",
    "\n",
    "    \"Return the discounted rewards for a trajectory.\"\n",
    "\n",
    "    new_rewards = [float(rewards[-1])]\n",
    "    for i in reversed(range(len(rewards)-1)):\n",
    "        new_rewards.append(rewards[i] + gamma * new_rewards[-1])\n",
    "    return np.array(new_rewards[::-1])\n",
    "\n",
    "def calculate_gaes(rewards, values, gamma=0.99, decay=0.97):\n",
    "    \"\"\"\n",
    "    rewards: torch tensor (T,)\n",
    "    values: torch tensor (T,)\n",
    "    \"\"\"\n",
    "    gaes = torch.zeros_like(rewards)\n",
    "    next_value = 0\n",
    "    gae = 0\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        gae = delta + gamma * decay * gae\n",
    "        gaes[t] = gae\n",
    "        next_value = values[t]\n",
    "\n",
    "    return gaes\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ff33391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, env, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Performs a single rollout.\n",
    "    Returns training data in the shape (n_steps, observation_dim)\n",
    "    and the cumulative reward.\n",
    "    \"\"\"\n",
    "    ### Create data storage\n",
    "    train_data = [[], [], [], [], []]  # obs, actions, rewards, values, act_log_probs \n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    ep_reward = 0\n",
    "    for _ in range(max_steps):\n",
    "        obs_t = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "        logits, val = model(obs_t)\n",
    "        act_distribution = Categorical(logits=logits)\n",
    "        act = act_distribution.sample()\n",
    "        act_log_prob = act_distribution.log_prob(act).item()\n",
    "\n",
    "        act, val = act.item(), val\n",
    "\n",
    "        next_obs, reward, terminated, truncated, _ = env.step(act)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        ### Record data for training\n",
    "        for i , item in enumerate((obs, act, reward, val, act_log_prob)):\n",
    "            train_data[i].append(item)\n",
    "\n",
    "        obs = next_obs\n",
    "        ep_reward += reward\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    ### Do train data filtering\n",
    "    train_data[2] = torch.tensor(train_data[2], dtype=torch.float32, device=DEVICE)\n",
    "    train_data[3] = torch.stack(train_data[3]).squeeze(-1)\n",
    "\n",
    "    train_data[3] = calculate_gaes(train_data[2], train_data[3])\n",
    "\n",
    "    return train_data, ep_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e183a5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "model = ActorCritic(env.observation_space.shape[0], env.action_space.n).to(DEVICE)\n",
    "train_data, reward = rollout(model, env) # Test rollout function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f61770c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Init PPO trainer and parameters\n",
    "# Define hyperparameters\n",
    "n_episodes = 200\n",
    "print_freq = 20\n",
    "\n",
    "ppo = PPOTrainer(\n",
    "    model,\n",
    "    policy_lr=3e-4,\n",
    "    value_lr=1e-3,\n",
    "    target_kl_div=0.02,\n",
    "    max_policy_train_iters=40,\n",
    "    value_train_iters=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6e7e9c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, Reward: 63.85\n",
      "Episode: 40, Reward: 140.15\n",
      "Episode: 60, Reward: 152.9\n",
      "Episode: 80, Reward: 232.15\n",
      "Episode: 100, Reward: 72.75\n",
      "Episode: 120, Reward: 180.15\n",
      "Episode: 140, Reward: 169.55\n",
      "Episode: 160, Reward: 224.55\n",
      "Episode: 180, Reward: 244.0\n",
      "Episode: 200, Reward: 222.25\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "ep_rewards = []\n",
    "for episode_idx in range(n_episodes):\n",
    "    # Perform rollout\n",
    "    train_data, reward = rollout(model, env)\n",
    "    ep_rewards.append(reward)\n",
    "\n",
    "    # Data formatting for training would go here\n",
    "\n",
    "    # Shuffle\n",
    "    permute_idx = np.random.permutation(len(train_data[0]))\n",
    "      \n",
    "    # Policy Data\n",
    "    obs = torch.tensor(np.array(train_data[0])[permute_idx], dtype=torch.float32).to(DEVICE)\n",
    "    acts = torch.tensor(np.array(train_data[1])[permute_idx], dtype=torch.int64).to(DEVICE)\n",
    "    gaes = train_data[3].detach()[permute_idx].to(DEVICE)\n",
    "    act_log_probs = torch.tensor(np.array(train_data[4])[permute_idx], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "\n",
    "    # Value Data\n",
    "    returns = discount_rewards(train_data[2])[permute_idx]\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "    # Train policy and value networks\n",
    "    ppo.train_policy(obs, acts, act_log_probs, gaes)\n",
    "    ppo.train_value(obs, returns.unsqueeze(-1))\n",
    "\n",
    "    # Logging\n",
    "    if (episode_idx + 1) % print_freq == 0:\n",
    "        print(f'Episode: {episode_idx + 1}, Reward: {np.mean(ep_rewards[-print_freq:])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
